{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "In this notebook, we analyse the (synthetic) data from our ficticious bank account. It is provided in CSV format and our goal is to create a graphical overview over our monthly expenses by category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from collections import OrderedDict, defaultdict\n",
    "import sqlite3 as sql\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from calendar import monthrange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring unknown data\n",
    "\n",
    "Every bank provides account statements in a different format. Panda's read_csv function can try to find certain properties of how the data is saved on its own, one example being the separator, but we will have to find the structure ourselves. So, as a first step, it makes sense to simply plot the first few rows and see what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"money_bin_201511_2015121.csv\")\n",
    "display(data.loc[range(10), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to deal with the datatype of the columns. The date column should ideally contain datetime objects and the amount column should be typed as float. Let's see if that is true and adjust the types if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "for i in list(data.columns):\n",
    "    print(\"Column {0} \\t\\t is of type {1}\".format(i, data[i].dtype))\n",
    "    \n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "data[\"ID\"] = data[\"ID\"].apply(lambda x: int(x) if x != \"\" else 0)\n",
    "\n",
    "print(\"--------------------\")\n",
    "for i in list(data.columns):\n",
    "    print(\"Column {0} is of type {1}\".format(i, data[i].dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "For our analysis, we do not need to column 'ID'. Therefore, we have the possibility of merging it with the subject or dropping it altogether. In this example, we will do both, as we first merge the ID column into subject and then drop it. In a final step, the columns are reordered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Subject\"] = data[[\"Subject\", \"ID\"]].apply(lambda x: \" \".join([str(y) for y in x]), axis=1)\n",
    "data.drop([\"ID\"], 1, inplace=True)\n",
    "data = data[[\"Date\", \"Recipient\", \"Subject\", \"Amount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"Date\", \"Recipient\", \"Subject\", \"Amount\"]]\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling\n",
    "\n",
    "In a real world setting, we are not going to have only one csv file but typically many files that ideally do not overlap but cannot be guaranteed to do so. We therefore assign a unique identifier to each entry. A simple way to do that is to join all entries into a single string and calculate its md5 checksum. The problem with this approach is that when a payment is made twice on the same day, say two cups of coffee at different times, this approach will fail. Therefore, we include the relative index of payment which makes the hashsum unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_frame = pd.DataFrame(     \n",
    "    columns=[\"Date\", \"Recipient\", \"Subject\", \"Amount\", \"Category\", \"SimpleChecksum\", \"Checksum\"],\n",
    "    index=np.arange(data.shape[0]))\n",
    "\n",
    "idx = 0\n",
    "for d in pd.date_range(data[\"Date\"].min(), data[\"Date\"].max(), freq=\"1d\"):\n",
    "    relevant_data = data.loc[data[\"Date\"] == d, :]\n",
    "\n",
    "    for rd_idx, rd in enumerate(relevant_data.iterrows()):\n",
    "        checksum = hashlib.md5(str(rd_idx).join([str(s) for s in rd[1]]).encode()).hexdigest()                \n",
    "        simple_checksum = hashlib.md5(\"\".join([str(s) for s in rd[1]]).encode()).hexdigest()                \n",
    "        entry = [str(rd[1][\"Date\"]), rd[1][\"Recipient\"], rd[1][\"Subject\"], float(rd[1][\"Amount\"]), \"\", simple_checksum, checksum]\n",
    "        insert_frame.loc[idx, :] = entry\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data\n",
    "\n",
    "The last step in processing the new dataset is to assign categories to those payments that we have encountered previously. In this synthetic example, we know the categories quite well, but in a real world dataset, there might be some entries that need manual categorization at least once. Examples include cash expenses, new restaurants and new stores.\n",
    "\n",
    "On a technical level, we are simply going to check if a certain string is present in either the recipient or the subject or if a combination is true. We are going to save this in an OrderedDict. This has the advantage of being able to set up some broad rules like anything containing the string \"Mart\" is categorized as \"Grocery Store\" that can be overriden with a more specific rule saying for example that \"Martin's Burger Place\" is categorized as \"Restaurant\". An ordinary python dict in versions <3.6 do not care about the order, which may lead to wrong classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expns = OrderedDict()\n",
    "\n",
    "expns[\"CoffeeBrothers\"] = (\"Coffee\", \"CoffeeBrothers\", \"Expenses::Restaurant\")\n",
    "expns[\"BeansBeansBeans\"] = (\"Coffee\", \"BeansBeansBeans\", \"Expenses::Restaurant\")\n",
    "expns[\"Who likes dough?\"] = (\"Bakery\", \"Who likes dough?\", \"Expenses::Restaurant\")\n",
    "expns[\"BestBakers\"] = (\"Bakery\", \"BestBakers\", \"Expenses::Restaurant\")\n",
    "expns[\"Cuisine Francaise\"] = (\"Restaurant\", \"Cuisine Francaise\", \"Expenses::Restaurant\")\n",
    "expns[\"Best Burgers\"] = (\"Restaurant\", \"Best Burgers\", \"Expenses::Restaurant\")\n",
    "expns[\"Gimme Gyros\"] = (\"Restaurant\", \"Gimme Gyros\", \"Expenses::Restaurant\")\n",
    "expns[\"Bernhard Bratwurst\"] = (\"Restaurant\", \"Bernhard Bratwurst\", \"Expenses::Restaurant\")\n",
    "expns[\"Trattoria Accento\"] = (\"Restaurant\", \"Trattoria Accento\", \"Expenses::Restaurant\")\n",
    "expns[\"Convencience\"] = (\"Convenience Store\", \"Convenience\", \"Expenses::Convenience Stores\")\n",
    "expns[\"Plumber\"] = (\"Plumber\", \"\", \"Expenses::Living::Repairs\")\n",
    "expns[\"DIY Furniture\"] = (\"Furniture\", \"DIY Furniture\", \"Expenses::Living::Furniture\")\n",
    "expns[\"Electrobuddy\"] = (\"Electronics\", \"Electrobuddy\", \"Expenses::Living::Electronics\")\n",
    "expns[\"Farmer's Market\"] = (\"Groceries\", \"Farmer's Market\", \"Expenses::Living::Groceries\")\n",
    "expns[\"Franny's Fantastic Food\"] = (\"Groceries\", \"Franny's Fantastic Food\", \"Expenses::Living::Groceries\")\n",
    "expns[\"Super Foods Market\"] = (\"Groceries\", \"Super Foods Market\", \"Expenses::Living::Groceries\")\n",
    "expns[\"Gas\"] = (\"Car\", \"Gas\", \"Expenses::Car\")\n",
    "expns[\"Clothes left and right\"] = (\"Clothes\", \"Clothes left and right\", \"Expenses::Clothes\")\n",
    "expns[\"Rent\"] = (\"Rent\", \"\", \"Expenses::Living::Rent\")\n",
    "expns[\"Car Loan\"] = (\"Car Loan\", \"\", \"Expenses::Car\")\n",
    "expns[\"Insurances\"] = (\"Insurance\", \"\", \"Expenses::Insurance\")\n",
    "expns[\"Phone\"] = (\"Phone\", \"\", \"Expenses::Communication\")\n",
    "\n",
    "def map_entries(entry, categories):  \n",
    "    \n",
    "    # check if a category has been assigned already to avoid overwriting user choices\n",
    "    try:\n",
    "        if len(entry[\"Category\"]) > 3: return entry\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    recipient = str(entry[\"Recipient\"]).lower()\n",
    "    subject = str(entry[\"Subject\"]).lower()\n",
    "    for k in categories.keys():\n",
    "        if k.find(\"|\") == -1:\n",
    "            if k.lower() in recipient or k.lower() in subject:\n",
    "                if len(categories[k]) == 3:\n",
    "                    entry[\"Recipient\"], entry[\"Subject\"], entry[\"Category\"] = categories[k]\n",
    "                elif len(categories[k]) > 3:\n",
    "                    entry[\"Category\"] = categories[k]\n",
    "                break\n",
    "        else:\n",
    "            r, s = k.lower().split(\"|\")\n",
    "            if r in recipient and s in subject:\n",
    "                entry[\"Recipient\"], entry[\"Subject\"], entry[\"Category\"] = categories[k]\n",
    "                break\n",
    "    return entry\n",
    "\n",
    "data = data.apply(lambda x: map_entries(x, expns), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together and dumping to sqlite\n",
    "\n",
    "Here's how the routine would look in practice. We iterate over all possible csv files and look for overlap before inserting. The results are saved to an sqlite database which makes it easy to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(     \n",
    "    columns=[\"Date\", \"Recipient\", \"Subject\", \"Amount\", \"Category\", \"SimpleChecksum\", \"Checksum\"])\n",
    "sql_file = \"money_bin.db\"\n",
    "\n",
    "if os.path.exists(sql_file):\n",
    "    conn = sql.connect(sql_file)\n",
    "    curs = conn.cursor()\n",
    "    sql_data = pd.read_sql_query(\"SELECT * FROM money_bin\", conn)\n",
    "    df = df.append(sql_data)\n",
    "    print(\"Transactions present in database:\", len(sql_data))\n",
    "    curs.close()\n",
    "    conn.close()\n",
    "\n",
    "for csv_file in glob(\"money_bin*.csv\"):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data.fillna(\"\", inplace=True)\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data[\"ID\"] = data[\"ID\"].apply(lambda x: int(x) if x != \"\" else 0)\n",
    "    data[\"Subject\"] = data[[\"Subject\", \"ID\"]].apply(lambda x: \" \".join([str(y) for y in x]), axis=1)\n",
    "    data.drop([\"ID\"], 1, inplace=True)\n",
    "    data = data[[\"Date\", \"Recipient\", \"Subject\", \"Amount\"]]    \n",
    "\n",
    "    insert_frame = pd.DataFrame(     \n",
    "        columns=[\"Date\", \"Recipient\", \"Subject\", \"Amount\", \"Category\", \"SimpleChecksum\", \"Checksum\"],\n",
    "        index=np.arange(data.shape[0]))\n",
    "    \n",
    "    idx = 0\n",
    "    for d in pd.date_range(data[\"Date\"].min(), data[\"Date\"].max(), freq=\"1d\"):\n",
    "        relevant_data = data.loc[data[\"Date\"] == d, :]\n",
    "\n",
    "        for rd_idx, rd in enumerate(relevant_data.iterrows()):\n",
    "            checksum = hashlib.md5(str(rd_idx).join([str(s) for s in rd[1]]).encode()).hexdigest()                \n",
    "            simple_checksum = hashlib.md5(\"\".join([str(s) for s in rd[1]]).encode()).hexdigest()                \n",
    "            entry = [str(rd[1][\"Date\"]), rd[1][\"Recipient\"], rd[1][\"Subject\"], float(rd[1][\"Amount\"]), \"\", simple_checksum, checksum]\n",
    "            insert_frame.loc[idx, :] = entry\n",
    "            idx += 1\n",
    "\n",
    "    insert_frame = insert_frame.apply(lambda x: map_entries(x, expns), 1)            \n",
    "    \n",
    "    if len(df) != 0:\n",
    "        duplicates = -insert_frame[\"Checksum\"].isin(df[\"Checksum\"])    \n",
    "        print(\"Unique transactions: \", len(insert_frame[duplicates]))\n",
    "        df = df.append(insert_frame[duplicates])\n",
    "    else:\n",
    "        df = df.append(insert_frame)\n",
    "\n",
    "\n",
    "    print(\"Updated number of transactions: \", len(df))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "\n",
    "conn = sql.connect(sql_file)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS money_bin (Date TEXT, Recipient TEXT, Subject TEXT, Amount REAL, Category TEXT, SimpleChecksum TEXT, Checksum TEXT)\")\n",
    "cursor.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS money_bin_idx ON money_bin(Checksum)\")\n",
    "columns = \",\".join([\"'{0}'\".format(c) for c in df.columns.tolist()])\n",
    "cursor.executemany(\"INSERT OR IGNORE INTO money_bin ({0}) VALUES (?, ?, ?, ?, ?, ?, ?)\".format(columns), df.values.tolist())\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel interaction\n",
    "\n",
    "Pandas provides excel read and write support that is just as easy to use as csv export. In combination with *xlwings*, we have a powerful tool at our disposal to analyse and manipulate data in real time. The following routines take care of converting SQL data to Excel and back. If the excel sheet money_bin.xlsx is open, you can manipulate the data in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlwings as xw\n",
    "\n",
    "def xlsx2sql():\n",
    "    col_names = [\"Date\", \"Recipient\", \"Subject\", \"Amount\", \"Category\", \"SimpleChecksum\", \"Checksum\"]\n",
    "    sql_names =\",\".join([n + \"=?\" for n in col_names])\n",
    "    dfs = [pd.read_excel(\"money_bin.xlsx\", str(y)) for y in [2015, 2016, 2017]]\n",
    "    xl = pd.concat(dfs).fillna(\"\")\n",
    "    xl[\"Date\"] = xl[\"Date\"].apply(lambda x: str(pd.to_datetime(x))).values\n",
    "    xl = xl.apply(lambda x: map_entries(x, expns), 1)\n",
    "\n",
    "    conn = sql.connect(\"money_bin.db\")\n",
    "    cursor = conn.cursor()\n",
    "    for d in xl.iterrows():\n",
    "        cursor.execute(\"UPDATE OR IGNORE money_bin SET {0} WHERE Checksum='{1}'\".format(sql_names, d[1].values[-1]), d[1].values)\n",
    "        conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "def sql2xlsx():\n",
    "    conn = sql.connect(\"money_bin.db\")\n",
    "    columns = [\"Date\", \"Recipient\", \"Subject\", \"Amount\",\"Category\", \"SimpleChecksum\", \"Checksum\"]    \n",
    "    excel_file = pd.ExcelWriter(\"money_bin.xlsx\", engine=\"xlsxwriter\")\n",
    "\n",
    "    for year in range(2015, 2018):\n",
    "        entries = pd.read_sql_query(\"SELECT * FROM money_bin WHERE Date>='{0}' and Date<='{1}' ORDER BY Date\".format(datetime(year, 1, 1), datetime(year, 12, 31)), conn)\n",
    "        entries = entries.apply(lambda x: map_entries(x, expns), 1)\n",
    "        entries.to_excel(excel_file, str(year), index=False)\n",
    "\n",
    "        try:\n",
    "            wb = xw.Book(\"money_bin.xlsx\".format(account))\n",
    "            sht = wb.sheets[str(year)]\n",
    "            sht.range(\"A2:I16384\").value = \"\"\n",
    "            sht.range(\"A2\").value = entries.values        \n",
    "            sht.range(\"A1\").value = columns\n",
    "        except:\n",
    "            print(\"Error setting entries on the fly\")\n",
    "            pass\n",
    "    conn.close()\n",
    "    excel_file.save()\n",
    "    \n",
    "def show_uncategorized():\n",
    "    conn = sql.connect(\"money_bin.db\")\n",
    "    columns = [\"Date\", \"Recipient\", \"Subject\", \"Amount\", \"Category\", \"SimpleChecksum\", \"Checksum\"]\n",
    "    for year in range(2015, 2018):\n",
    "        entries = pd.read_sql_query(\"SELECT * FROM money_bin WHERE Category='' AND Date>='{0}' and Date<='{1}' ORDER BY Date\".format(datetime(year, 1, 1), datetime(year, 12, 31)), conn)\n",
    "        try:\n",
    "            wb = xw.Book(\"money_bin.xlsx\")\n",
    "            sht = wb.sheets[str(year)]\n",
    "            sht.range(\"A2:I16384\").value = \"\"\n",
    "            sht.range(\"A2\").value = entries.values        \n",
    "            sht.range(\"A1\").value = columns\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            pass  \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save SQL to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2xlsx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write changes back to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx2sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show only items that are uncategorized in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_uncategorized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines for getting data and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all data for years[0].months[0] to years[1].months[1]\n",
    "def get_data(years, months):\n",
    "    try:\n",
    "        conn = sql.connect(\"money_bin.db\")\n",
    "        query = \"SELECT * FROM money_bin WHERE date>='{0}' and date<='{1}'\".format(datetime(years[0], months[0], 1, 0, 0), datetime(years[1], months[1], monthrange(years[1], months[1])[1], 0, 0))\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    except:\n",
    "        print(\"Getting data failed\")\n",
    "        print(sys.exc_info())\n",
    "    return df.dropna()\n",
    "\n",
    "# Get all data for years[0].months[0] to years[1].months[1]\n",
    "# by category\n",
    "def get_data_category(years, months, category):\n",
    "    try:\n",
    "        conn = sql.connect(\"money_bin.db\".format(db))\n",
    "        query = \"SELECT * FROM money_bin WHERE Category LIKE '{0}%' AND date>='{1}}' and date<='{2}'\".format(category, datetime(years[0], months[0], 1, 0, 0), datetime(years[1], months[1], monthrange(years[1], months[1])[1], 0, 0))\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df = df.append(sql_entries)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    except:\n",
    "        print(\"Getting data by category failed\")\n",
    "        \n",
    "    return df.dropna()\n",
    "\n",
    "# Get a list of the n_subcategories most important subcategories\n",
    "def get_most_relevant_subcategories(category, n_subcategories, years, months, ignore=[]):\n",
    "    df = get_data(years, months)   \n",
    "    counts = defaultdict(lambda:float(0))\n",
    "    for d in df.loc[df.Category.str.startswith(category), [\"Category\", \"Amount\"]].iterrows():\n",
    "        c = d[1][\"Category\"].replace(category, \"\").split(\"::\")    \n",
    "        if len(c) == 1:\n",
    "            counts[category] -= d[1][\"Amount\"]\n",
    "        else:\n",
    "            if c[1] in ignore:\n",
    "                continue                        \n",
    "            counts[c[1]] -= d[1][\"Amount\"]\n",
    "\n",
    "    counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    labels = [c[0] for c in counts[0:n_subcategories - 1]]\n",
    "    return labels\n",
    "\n",
    "# Return a tuple of account names and balances for the n_subcategories accounts with the largest balance\n",
    "def get_amounts(category, n_subcategories, years, months, ignore=[]):\n",
    "    df = get_data(years, months)\n",
    "    \n",
    "    counts = defaultdict(lambda:float(0))\n",
    "    for d in df.loc[df.Category.str.startswith(category), [\"Category\", \"Amount\"]].iterrows():\n",
    "        c = d[1][\"Category\"].replace(category, \"\").split(\"::\")    \n",
    "        if len(c) == 1:\n",
    "            counts[category] -= d[1][\"Amount\"]\n",
    "        else:\n",
    "            if c[1] in ignore:\n",
    "                continue                                    \n",
    "            counts[c[1]] -= d[1][\"Amount\"]\n",
    "\n",
    "    counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    labels = [c[0] for c in counts[0:n_subcategories - 1]]\n",
    "    labels += [\"Miscellaneous\"]\n",
    "    amounts = [c[1] for c in counts[0:n_subcategories - 1]]\n",
    "    amounts += [sum([c[1] for c in counts[n_subcategories - 1:]])]\n",
    "    return labels, amounts\n",
    "\n",
    "# Return a tuple of account names and balances for desired subcategories\n",
    "def get_amounts_by_subcategories(category, subcategories, years, months, ignore=[]):\n",
    "    df = get_data(years, months)\n",
    "    \n",
    "    counts = OrderedDict([(s, 0.) for s in subcategories])\n",
    "    counts[\"Miscellaneous\"] = 0.\n",
    "    \n",
    "    for d in df.loc[df.Category.str.startswith(category), [\"Category\", \"Amount\"]].iterrows():\n",
    "        c = d[1][\"Category\"].replace(category, \"\").split(\"::\")        \n",
    "        if c[1] in subcategories:\n",
    "            counts[c[1]] -= d[1][\"Amount\"]\n",
    "        else:\n",
    "            if c[1] in ignore:\n",
    "                continue                        \n",
    "            counts[\"Miscellaneous\"] -= d[1][\"Amount\"]\n",
    "\n",
    "    return list(counts.keys()), list(counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of the most relevant categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frst_date = pd.datetime(2015, 1, 1)\n",
    "last_date = pd.datetime(2017, 7, 31)\n",
    "\n",
    "categories = get_most_relevant_subcategories(\"Expenses\", 24, [frst_date.year, last_date.year], [frst_date.month, last_date.month])\n",
    "print(categories)\n",
    "amounts = OrderedDict([(s, []) for s in categories])\n",
    "amounts[\"Miscellaneous\"] = []\n",
    "date_labels = []\n",
    "\n",
    "for date in pd.date_range(frst_date, last_date, freq=\"MS\"):\n",
    "    date_labels.append(date.strftime(\"%B %Y\"))\n",
    "    x, y = get_amounts_by_subcategories(\"Expenses\", categories, [date.year, date.year], [date.month, date.month])\n",
    "    for cat, am in zip(x, y):\n",
    "        amounts[cat].append(am)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = len(pd.date_range(frst_date, last_date, freq=\"MS\"))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "budget_frame = pd.DataFrame()\n",
    "\n",
    "baseline = np.zeros(months)\n",
    "for cat, ys in amounts.items(): \n",
    "    plt.bar(range(months), ys, bottom=baseline, label=cat)\n",
    "    baseline += ys\n",
    "    budget_frame[cat] = ys\n",
    "    \n",
    "plt.legend()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], title='Line', loc='upper left')\n",
    "\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "try:\n",
    "    wb = xw.Book(\"expenses.xlsx\")\n",
    "    sht = wb.sheets[\"Expenses\"]\n",
    "    sht.range(\"A2:ZZ16384\").value = \"\"\n",
    "    sht.range(\"B2\").value = budget_frame.transpose()\n",
    "    sht.range(\"C2\").value = date_labels\n",
    "    sht.range(\"C{0}\".format(budget_frame.shape[1] + 4)).value = budget_frame.sum(1).values\n",
    "    sht.range(\"A3\").expand(\"down\").value = [[v] for v in budget_frame.mean(0).values]\n",
    "    sht.range(\"A{0}\".format(budget_frame.shape[1] + 4)).value = sum(budget_frame.mean(0).values)\n",
    "    sht.pictures.add(fig, name=\"overview\", update=True)\n",
    "except:\n",
    "    print(\"Could not add values to sheet\")\n",
    "    print(sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "I hope that you are convinced that handling big datasets in pandas is really easy. A next step would be to take an actual dataset from your bank and see if you can analyse and if the simple keyword search suffices for categorizing transactions. You could also add the functionality of handling multiple accounts and generate more detailed statements based on category and recipient / subject if you want to have a more fine-grained picture of where your money actually flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
